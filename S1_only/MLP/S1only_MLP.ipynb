{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"},
      "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAelB_MH6iJ2",
        "outputId": "51e46055-81a5-4b36-c66a-8801339f03dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ÏÑ§Ïπò (ÏµúÏ¥à 1ÌöåÎßå ÌïÑÏöî)\n",
        "# =========================================================\n",
        "!pip -q install optuna tqdm ipywidgets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7FOCcInJoMf",
        "outputId": "68c6af01-4a40-444d-9b31-a0d71fba18d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-fold"
      ],
      "metadata": {
        "id": "gF0vBbYBHOPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "# =========================================================\n",
        "import os, json, warnings, math, random, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, roc_auc_score, f1_score, accuracy_score,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# =========================================================\n",
        "# ÌååÏùº Í≤ΩÎ°ú\n",
        "# =========================================================\n",
        "FILE_PATHS = {\n",
        "    \"Amazon\":  '/content/drive/MyDrive/1014/data/new_amazon.csv',\n",
        "    \"Coursera\":'/content/drive/MyDrive/1014/data/new_coursera.csv',\n",
        "    \"Audible\": '/content/drive/MyDrive/1014/data/new_audible.csv',\n",
        "    \"Hotel\":   '/content/drive/MyDrive/1014/data/new_hotel.csv'\n",
        "}\n",
        "\n",
        "# =========================================================\n",
        "# S1 ÌîºÏ≤ò (Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©, ÏóÜÎäî Ïª¨ÎüºÏùÄ ÏûêÎèô Ï†úÏô∏)\n",
        "# =========================================================\n",
        "S1_FEATURES = {\n",
        "    \"Amazon\":  ['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Price','Text_Length','Valence','Arousal','Title_Length','Num_of_Ratings','Is_Photo','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth'],\n",
        "    \"Coursera\":['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Num_of_Reviews','Num_of_Enrolled','Num_of_top_instructor_courses','Num_of_top_instructor_learners','Text_Length','Valence','Arousal','Num_of_Ratings','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth'],\n",
        "    \"Audible\": ['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Text_Length','Valence','Arousal','Title_Length','Num_of_Ratings','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth'],\n",
        "    \"Hotel\":   ['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Text_Length','Valence','Arousal','Title_Length','Num_of_Ratings','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth','Is_Photo','Hotel_Grade','Employee_Friendliness_Score','Facility_Score','Cleanliness_Score','Comfort_Score','Value_For_Money_Score','Location_Score']\n",
        "}\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "TARGET_COLUMN     = 'binary_helpfulness'\n",
        "TEST_SPLIT_RATIO  = 0.2\n",
        "RANDOM_STATE      = 42\n",
        "N_TRIALS          = 50\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Device = {DEVICE}\")\n",
        "\n",
        "def set_seed(seed=RANDOM_STATE):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(RANDOM_STATE)\n",
        "\n",
        "# =========================================================\n",
        "# Utils\n",
        "# =========================================================\n",
        "def _make_numeric_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    num = df.apply(pd.to_numeric, errors='coerce')\n",
        "    med = num.median()\n",
        "    return num.fillna(med)\n",
        "\n",
        "def find_best_threshold(y_true, prob):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, prob)\n",
        "    thresholds = np.concatenate([thresholds, [1.0]])\n",
        "    f1s = (2 * precision * recall) / np.clip(precision + recall, 1e-9, None)\n",
        "    idx = int(np.nanargmax(f1s))\n",
        "    return float(thresholds[idx]), float(f1s[idx])\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1,1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=256, depth=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers=[]; d=in_dim\n",
        "        for _ in range(depth):\n",
        "            layers += [nn.Linear(d, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            d = hidden_dim\n",
        "        layers += [nn.Linear(d, 1)]  # logits\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_one_model(X_tr, y_tr, X_va, y_va, params):\n",
        "    \"\"\"Ìïú Î≤àÏùò ÌïôÏäµ: Ï°∞Í∏∞Ï¢ÖÎ£å(early stopping)Î°ú best PRAUC Î™®Îç∏ Î∞òÌôò\"\"\"\n",
        "    batch_size   = params.get(\"batch_size\", 1024)\n",
        "    lr           = params.get(\"lr\", 1e-3)\n",
        "    weight_decay = params.get(\"weight_decay\", 1e-5)\n",
        "    hidden_dim   = params.get(\"hidden_dim\", 256)\n",
        "    depth        = params.get(\"depth\", 2)\n",
        "    dropout      = params.get(\"dropout\", 0.2)\n",
        "    max_epochs   = params.get(\"max_epochs\", 50)\n",
        "    patience     = params.get(\"patience\", 5)\n",
        "\n",
        "    model = MLP(X_tr.shape[1], hidden_dim, depth, dropout).to(DEVICE)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    crit  = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    dl_tr = DataLoader(NumpyDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    dl_va = DataLoader(NumpyDataset(X_va, y_va), batch_size=4096, shuffle=False, drop_last=False)\n",
        "\n",
        "    best_pr_auc = -1.0\n",
        "    best_state  = None\n",
        "    bad_epochs  = 0\n",
        "\n",
        "    for ep in range(1, max_epochs+1):\n",
        "        model.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
        "            logit = model(xb)\n",
        "            loss  = crit(logit, yb)\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            opt.step()\n",
        "\n",
        "        # ---- validation (PR_AUC)\n",
        "        model.eval(); probs=[]; ys=[]\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(DEVICE)\n",
        "                logit = model(xb)\n",
        "                prob = torch.sigmoid(logit).cpu().numpy().ravel()\n",
        "                probs.append(prob)\n",
        "                ys.append(yb.numpy().ravel())\n",
        "        probs = np.concatenate(probs); ys = np.concatenate(ys)\n",
        "        pr = average_precision_score(ys, probs)\n",
        "\n",
        "        # early stopping\n",
        "        if pr > best_pr_auc + 1e-6:\n",
        "            best_pr_auc = pr\n",
        "            best_state  = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad_epochs  = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                break\n",
        "\n",
        "    # load best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_pr_auc\n",
        "\n",
        "# =========================================================\n",
        "# Î©îÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏ (ÌîåÎû´Ìèº Îã®ÏúÑ)\n",
        "# =========================================================\n",
        "def run_s1_pipeline(platform, csv_path, features):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"‚ñ∂ Platform: {platform}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1) Îç∞Ïù¥ÌÑ∞ Î°úÎìú & Ï†ÑÏ≤òÎ¶¨\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert TARGET_COLUMN in df.columns, f\"{TARGET_COLUMN} ÏóÜÏùå\"\n",
        "    labels = df[TARGET_COLUMN].astype(int).values\n",
        "\n",
        "    exists = [c for c in features if c in df.columns]\n",
        "    if len(exists)==0:\n",
        "        raise ValueError(\"ÏÇ¨Ïö© Í∞ÄÎä•Ìïú S1 ÌîºÏ≤òÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "    X_all = _make_numeric_df(df[exists]).to_numpy()\n",
        "\n",
        "    # Stratified split\n",
        "    idx = np.arange(len(df))\n",
        "    tr_idx, te_idx = train_test_split(\n",
        "        idx, test_size=TEST_SPLIT_RATIO, random_state=RANDOM_STATE, stratify=labels\n",
        "    )\n",
        "    X_train_raw, X_test_raw = X_all[tr_idx], X_all[te_idx]\n",
        "    y_train, y_test = labels[tr_idx], labels[te_idx]\n",
        "\n",
        "    # Ïä§ÏºÄÏùºÎßÅ(MLP ÌïÑÏàò) ‚Äî train Í∏∞Ï§ÄÏúºÎ°ú fit\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train_raw)\n",
        "    X_test  = scaler.transform(X_test_raw)\n",
        "    print(f\"‚úÖ Train={len(y_train)}, Test={len(y_test)} | d={X_train.shape[1]}\")\n",
        "\n",
        "    # 2) Optuna (PR_AUC ÏµúÎåÄÌôî) ‚Äî 3-fold CV\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"hidden_dim\": trial.suggest_categorical(\"hidden_dim\", [128, 256, 512, 768]),\n",
        "            \"depth\": trial.suggest_int(\"depth\", 1, 4),\n",
        "            \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
        "            \"lr\": trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True),\n",
        "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-7, 1e-3, log=True),\n",
        "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [256, 512, 1024, 2048]),\n",
        "            \"max_epochs\": 50,\n",
        "            \"patience\": 5,\n",
        "        }\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
        "        pr_aucs = []\n",
        "        # Í∞Å foldÎßàÎã§ scaler Ïû¨Ï†ÅÏö©(Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Î∞©ÏßÄ)\n",
        "        for tr, va in skf.split(X_train, y_train):\n",
        "            # foldÎ≥ÑÎ°ú Îî∞Î°ú Ïä§ÏºÄÏùºÎü¨Î•º ÎßûÏ∂îÎäîÍ≤å ÏóÑÎ∞ÄÌïòÏßÄÎßå, ÏúÑÏóêÏÑú Ïù¥ÎØ∏ Ï†ÑÏ≤¥ trainÏóê ÎßûÏ∑ÑÏúºÎØÄÎ°ú\n",
        "            # ÏóÑÍ≤© Î™®ÎìúÎ°ú ÌïòÎ†§Î©¥ Îã§Ïùå Îëê Ï§Ñ ÏÇ¨Ïö©:\n",
        "            # _sc = StandardScaler().fit(X_train[tr]); X_tr = _sc.transform(X_train[tr]); X_va = _sc.transform(X_train[va])\n",
        "            X_tr, X_va = X_train[tr], X_train[va]\n",
        "            y_tr, y_va = y_train[tr], y_train[va]\n",
        "\n",
        "            model, val_pr = train_one_model(X_tr, y_tr, X_va, y_va, params)\n",
        "            pr_aucs.append(val_pr)\n",
        "\n",
        "            # Î©îÎ™®Î¶¨ tidy\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return float(np.mean(pr_aucs))\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    with tqdm(total=N_TRIALS, desc=f\"Optuna Tuning [{platform}]\", unit=\"trial\") as pbar:\n",
        "        def cb(study, trial):\n",
        "            pbar.update(1)\n",
        "        study.optimize(objective, n_trials=N_TRIALS, callbacks=[cb])\n",
        "\n",
        "    best_params = study.best_params\n",
        "    print(\"üß™ Best Params (MLP):\", best_params)\n",
        "\n",
        "    # 3) ÏµúÏ¢Ö ÌïôÏäµ (train ÎÇ¥Î∂Ä 10%Î•º valÎ°ú ÏÇ¨Ïö©Ìï¥ early stopping)\n",
        "    sub_tr, val = train_test_split(\n",
        "        np.arange(len(y_train)), test_size=0.1, random_state=RANDOM_STATE, stratify=y_train\n",
        "    )\n",
        "    X_sub, y_sub = X_train[sub_tr], y_train[sub_tr]\n",
        "    X_val, y_val = X_train[val],   y_train[val]\n",
        "\n",
        "    final_model, _ = train_one_model(X_sub, y_sub, X_val, y_val, {**best_params})\n",
        "\n",
        "    # 4) ÌèâÍ∞Ä + threshold\n",
        "    final_model.eval()\n",
        "    with torch.no_grad():\n",
        "        tr_prob = torch.sigmoid(final_model(torch.tensor(X_train, dtype=torch.float32).to(DEVICE))).cpu().numpy().ravel()\n",
        "        te_prob = torch.sigmoid(final_model(torch.tensor(X_test,  dtype=torch.float32).to(DEVICE))).cpu().numpy().ravel()\n",
        "\n",
        "    best_th, _ = find_best_threshold(y_train, tr_prob)\n",
        "    te_pred = (te_prob >= best_th).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": float(accuracy_score(y_test, te_pred)),\n",
        "        \"PR_AUC\":   float(average_precision_score(y_test, te_prob)),\n",
        "        \"ROC_AUC\":  float(roc_auc_score(y_test, te_prob)),\n",
        "        \"F1_score\": float(f1_score(y_test, te_pred)),\n",
        "        \"Best_Threshold\": float(best_th)\n",
        "    }\n",
        "    print(\"=== Test Metrics (MLP) ===\", metrics)\n",
        "\n",
        "    # 5) Ï†ÄÏû•\n",
        "    save_dir = f\"/content/drive/MyDrive/1014/result_mlp/{platform}\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # ÌôïÎ•†/ÏòàÏ∏°\n",
        "    pd.DataFrame({\n",
        "        \"index\": te_idx,\n",
        "        \"s1_pred_proba\": te_prob,\n",
        "        \"y_true\": y_test,\n",
        "        \"y_pred_at_best_th\": te_pred\n",
        "    }).to_csv(f\"{save_dir}/s1_pred_proba.csv\", index=False)\n",
        "\n",
        "    # Î©îÌä∏Î¶≠ + ÏÑ§Ï†ï\n",
        "    with open(f\"{save_dir}/results.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            **metrics,\n",
        "            \"features_used\": exists,\n",
        "            \"random_state\": RANDOM_STATE,\n",
        "            \"best_params\": best_params\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Î™®Îç∏ & Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•\n",
        "    torch.save(final_model.state_dict(), f\"{save_dir}/mlp_model.pt\")\n",
        "    with open(f\"{save_dir}/scaler.pkl\", \"wb\") as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    print(f\"üìÅ Results saved in {save_dir}\")\n",
        "    print(f\"üß† Model: {save_dir}/mlp_model.pt  |  Scaler: {save_dir}/scaler.pkl\")\n",
        "\n",
        "# =========================================================\n",
        "# Ï†ÑÏ≤¥ Ïã§Ìñâ (ÌîåÎû´Ìèº Î£®ÌîÑÎèÑ notebook tqdm)\n",
        "# =========================================================\n",
        "for platform, path in tqdm(FILE_PATHS.items(), desc=\"Ï†ÑÏ≤¥ ÌîåÎû´Ìèº ÏßÑÌñâ (MLP)\", unit=\"platform\"):\n",
        "    run_s1_pipeline(platform, path, S1_FEATURES[platform])"
      ],
      "metadata": {
        "id": "eypT3dG09jOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-fold"
      ],
      "metadata": {
        "id": "UKUkl6uDyVAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# MLP + Optuna + 5-Fold CV (scaler ÎàÑÏàò Î∞©ÏßÄ)\n",
        "# =========================================================\n",
        "import os, json, warnings, math, random, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, roc_auc_score, f1_score, accuracy_score,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "FILE_PATHS = {\n",
        "    #\"Amazon\":  '/content/drive/MyDrive/data/new_amazon.csv',\n",
        "    \"Coursera\":'/content/drive/MyDrive/1014/data/new_coursera.csv',\n",
        "    \"Audible\": '/content/drive/MyDrive/1014/data/new_audible.csv',\n",
        "    \"Hotel\":   '/content/drive/MyDrive/1014/data/new_hotel.csv'\n",
        "}\n",
        "\n",
        "S1_FEATURES = {\n",
        "    \"Amazon\":  ['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Price','Text_Length','Valence','Arousal','Title_Length','Num_of_Ratings','Is_Photo','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth'],\n",
        "    \"Coursera\":['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Num_of_Reviews','Num_of_Enrolled','Num_of_top_instructor_courses','Num_of_top_instructor_learners','Text_Length','Valence','Arousal','Num_of_Ratings','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth'],\n",
        "    \"Audible\": ['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Text_Length','Valence','Arousal','Title_Length','Num_of_Ratings','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth'],\n",
        "    \"Hotel\":   ['Average_Rating','Rating','Deviation_Of_Star_Ratings','Time_Lapsed','Text_Length','Valence','Arousal','Title_Length','Num_of_Ratings','Flesch_Reading_Ease','FOG_Index','Sentiment_Score','new_depth','new_breadth','Is_Photo','Hotel_Grade','Employee_Friendliness_Score','Facility_Score','Cleanliness_Score','Comfort_Score','Value_For_Money_Score','Location_Score']\n",
        "}\n",
        "\n",
        "TARGET_COLUMN     = 'binary_helpfulness'\n",
        "TEST_SPLIT_RATIO  = 0.2\n",
        "RANDOM_STATE      = 42\n",
        "N_TRIALS          = 50\n",
        "N_SPLITS          = 5   # ‚úÖ 5-fold CV\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Device = {DEVICE}\")\n",
        "\n",
        "def set_seed(seed=RANDOM_STATE):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "set_seed(RANDOM_STATE)\n",
        "\n",
        "# =========================================================\n",
        "# Utils\n",
        "# =========================================================\n",
        "def _make_numeric_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    num = df.apply(pd.to_numeric, errors='coerce')\n",
        "    med = num.median()\n",
        "    return num.fillna(med)\n",
        "\n",
        "def find_best_threshold(y_true, prob):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, prob)\n",
        "    thresholds = np.concatenate([thresholds, [1.0]])\n",
        "    f1s = (2 * precision * recall) / np.clip(precision + recall, 1e-9, None)\n",
        "    idx = int(np.nanargmax(f1s))\n",
        "    return float(thresholds[idx]), float(f1s[idx])\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1,1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=256, depth=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers=[]; d=in_dim\n",
        "        for _ in range(depth):\n",
        "            layers += [nn.Linear(d, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            d = hidden_dim\n",
        "        layers += [nn.Linear(d, 1)]  # logits\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "def train_one_model(X_tr, y_tr, X_va, y_va, params):\n",
        "    batch_size   = params.get(\"batch_size\", 1024)\n",
        "    lr           = params.get(\"lr\", 1e-3)\n",
        "    weight_decay = params.get(\"weight_decay\", 1e-5)\n",
        "    hidden_dim   = params.get(\"hidden_dim\", 256)\n",
        "    depth        = params.get(\"depth\", 2)\n",
        "    dropout      = params.get(\"dropout\", 0.2)\n",
        "    max_epochs   = params.get(\"max_epochs\", 50)\n",
        "    patience     = params.get(\"patience\", 5)\n",
        "\n",
        "    model = MLP(X_tr.shape[1], hidden_dim, depth, dropout).to(DEVICE)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    crit  = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    dl_tr = DataLoader(NumpyDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True)\n",
        "    dl_va = DataLoader(NumpyDataset(X_va, y_va), batch_size=4096, shuffle=False)\n",
        "\n",
        "    best_pr_auc = -1.0\n",
        "    best_state  = None\n",
        "    bad_epochs  = 0\n",
        "\n",
        "    for ep in range(1, max_epochs+1):\n",
        "        model.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logit = model(xb)\n",
        "            loss  = crit(logit, yb)\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            opt.step()\n",
        "\n",
        "        # validation (PR_AUC)\n",
        "        model.eval(); probs=[]; ys=[]\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(DEVICE)\n",
        "                prob = torch.sigmoid(model(xb)).cpu().numpy().ravel()\n",
        "                probs.append(prob); ys.append(yb.numpy().ravel())\n",
        "        probs = np.concatenate(probs); ys = np.concatenate(ys)\n",
        "        pr = average_precision_score(ys, probs)\n",
        "\n",
        "        if pr > best_pr_auc + 1e-6:\n",
        "            best_pr_auc = pr\n",
        "            best_state  = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad_epochs  = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience: break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_pr_auc\n",
        "\n",
        "# =========================================================\n",
        "# Main Pipeline\n",
        "# =========================================================\n",
        "def run_s1_pipeline(platform, csv_path, features):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"‚ñ∂ Platform: {platform}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert TARGET_COLUMN in df.columns\n",
        "    labels = df[TARGET_COLUMN].astype(int).values\n",
        "\n",
        "    exists = [c for c in features if c in df.columns]\n",
        "    if len(exists)==0: raise ValueError(\"ÏÇ¨Ïö© Í∞ÄÎä•Ìïú S1 ÌîºÏ≤ò ÏóÜÏùå\")\n",
        "    X_all = _make_numeric_df(df[exists]).to_numpy()\n",
        "\n",
        "    idx = np.arange(len(df))\n",
        "    tr_idx, te_idx = train_test_split(idx, test_size=TEST_SPLIT_RATIO, random_state=RANDOM_STATE, stratify=labels)\n",
        "    X_train_raw, X_test_raw = X_all[tr_idx], X_all[te_idx]\n",
        "    y_train, y_test = labels[tr_idx], labels[te_idx]\n",
        "\n",
        "    # --- Optuna objective (5-fold CV, foldÎ≥Ñ scaler fit) ---\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"hidden_dim\":   trial.suggest_categorical(\"hidden_dim\", [128, 256, 512, 768]),\n",
        "            \"depth\":        trial.suggest_int(\"depth\", 1, 4),\n",
        "            \"dropout\":      trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
        "            \"lr\":           trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True),\n",
        "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-7, 1e-3, log=True),\n",
        "            \"batch_size\":   trial.suggest_categorical(\"batch_size\", [256, 512, 1024, 2048]),\n",
        "            \"max_epochs\":   50,\n",
        "            \"patience\":     5,\n",
        "        }\n",
        "        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "        pr_aucs=[]\n",
        "        for tr, va in skf.split(X_train_raw, y_train):\n",
        "            sc = StandardScaler().fit(X_train_raw[tr])\n",
        "            X_tr, X_va = sc.transform(X_train_raw[tr]), sc.transform(X_train_raw[va])\n",
        "            y_tr, y_va = y_train[tr], y_train[va]\n",
        "            model, val_pr = train_one_model(X_tr, y_tr, X_va, y_va, params)\n",
        "            pr_aucs.append(val_pr)\n",
        "            del model; torch.cuda.empty_cache()\n",
        "        return float(np.mean(pr_aucs))\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    with tqdm(total=N_TRIALS, desc=f\"Optuna Tuning [{platform}]\", unit=\"trial\") as pbar:\n",
        "        def cb(study, trial): pbar.update(1)\n",
        "        study.optimize(objective, n_trials=N_TRIALS, callbacks=[cb])\n",
        "\n",
        "    best_params = study.best_params\n",
        "    print(\"üß™ Best Params:\", best_params)\n",
        "\n",
        "    # --- ÏµúÏ¢Ö ÌïôÏäµ (train Ï†ÑÏ≤¥ + scaler) ---\n",
        "    scaler = StandardScaler().fit(X_train_raw)\n",
        "    X_train, X_test = scaler.transform(X_train_raw), scaler.transform(X_test_raw)\n",
        "\n",
        "    sub_tr, val = train_test_split(np.arange(len(y_train)), test_size=0.1, random_state=RANDOM_STATE, stratify=y_train)\n",
        "    X_sub, y_sub = X_train[sub_tr], y_train[sub_tr]\n",
        "    X_val, y_val = X_train[val],   y_train[val]\n",
        "\n",
        "    final_model, _ = train_one_model(X_sub, y_sub, X_val, y_val, {**best_params})\n",
        "\n",
        "    # --- ÌèâÍ∞Ä ---\n",
        "    final_model.eval()\n",
        "    with torch.no_grad():\n",
        "        tr_prob = torch.sigmoid(final_model(torch.tensor(X_train, dtype=torch.float32).to(DEVICE))).cpu().numpy().ravel()\n",
        "        te_prob = torch.sigmoid(final_model(torch.tensor(X_test,  dtype=torch.float32).to(DEVICE))).cpu().numpy().ravel()\n",
        "\n",
        "    best_th, _ = find_best_threshold(y_train, tr_prob)\n",
        "    te_pred = (te_prob >= best_th).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": float(accuracy_score(y_test, te_pred)),\n",
        "        \"PR_AUC\":   float(average_precision_score(y_test, te_prob)),\n",
        "        \"ROC_AUC\":  float(roc_auc_score(y_test, te_prob)),\n",
        "        \"F1_score\": float(f1_score(y_test, te_pred)),\n",
        "        \"Best_Threshold\": float(best_th)\n",
        "    }\n",
        "    print(\"=== Test Metrics ===\", metrics)\n",
        "\n",
        "    # --- Ï†ÄÏû• ---\n",
        "    save_dir = f\"/content/drive/MyDrive/1014/result_mlp5/{platform}\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    pd.DataFrame({\n",
        "        \"index\": te_idx,\n",
        "        \"s1_pred_proba\": te_prob,\n",
        "        \"y_true\": y_test,\n",
        "        \"y_pred_at_best_th\": te_pred\n",
        "    }).to_csv(f\"{save_dir}/s1_pred_proba.csv\", index=False)\n",
        "    with open(f\"{save_dir}/results.json\", \"w\") as f:\n",
        "        json.dump({**metrics,\"features_used\":exists,\"random_state\":RANDOM_STATE,\"best_params\":best_params}, f, indent=2)\n",
        "    torch.save(final_model.state_dict(), f\"{save_dir}/mlp_model.pt\")\n",
        "    with open(f\"{save_dir}/scaler.pkl\",\"wb\") as f: pickle.dump(scaler,f)\n",
        "\n",
        "    print(f\"üìÅ Saved in {save_dir}\")\n",
        "\n",
        "# =========================================================\n",
        "# Run All Platforms\n",
        "# =========================================================\n",
        "for platform, path in tqdm(FILE_PATHS.items(), desc=\"Ï†ÑÏ≤¥ ÌîåÎû´Ìèº ÏßÑÌñâ (MLP)\", unit=\"platform\"):\n",
        "    run_s1_pipeline(platform, path, S1_FEATURES[platform])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586,
          "referenced_widgets": [
            "2f9170286d444b7bacfc5d72ea3ccdd6",
            "224eded1395f4bd881d16245511779c1",
            "d3815f22a8dd41a78291e770fd8c21ee",
            "9ebef8d1fab24a568ea71a0f784e96ad",
            "073808ffaa1640888c6f6067b50d81c8",
            "d7045cc509694f17b63bc71e70bfcb43",
            "2536ede7dfb14322b3ad627fa11f60f2",
            "fafe3de5b08c4b0096587757ac705841",
            "642306646e904b1391f874c96b2a95be",
            "043ef09eea7646b18840937f5ddd5af8",
            "640c79ee595e4cd38b7748896307a2e7",
            "f1306666d4204a808d2bac14ca6c3c4a",
            "2c142e6269d64430803481731605cd6a",
            "2bcd1fde13964081a203781bb62dc53b",
            "982d103731724ac3a769ac4d9ecc77c4",
            "8b0fcf6fb904449c9230f8752452996d",
            "58674e492eb04e8c846064c077b6b9cc",
            "bf5456b74ae247f18cb8794ea6d6f829",
            "cafe6a4710be4925851bd3366223bdf7",
            "13a075dd8cab43b2a40d7b9cdaeb4954",
            "1395469eaef740d6b56c769c7a371577",
            "a4ad51a742f64ec588437e9e170a7c7c",
            "16bebce0cfce4f19b5c4c72335141502",
            "f46fd49510ed40ac8fcdac5791e131d5",
            "40310f578e694bbd9c66e7e52c1e9849",
            "6fcb18f26c9941fabc7bb1ea46ef6b75",
            "8265f6887a5046e28762302091f6bb59",
            "52fd7a3530bc4958893c0bd8fa23e8fc",
            "3e8167e336c847a1a28ccfed52515e95",
            "3a4ba12d81cd412ba6ad4d8af338b9d4",
            "d312f7b324a0482aadba706ec394332f",
            "a5a8f586ce5f4a04a77eec52eb07fa68",
            "20faa97eea554dacb94459bf1e6dae2b",
            "4ef73c8a6a20480ba59fa0ebb8c98937",
            "43b795bb2cea4311a5b445d04ce9e3bc",
            "2f5ee8a690d541cab0ff710f52aa27ed",
            "65a5d94a5f434b3ea10e4aa71bcccde1",
            "d9ef0e5129424662b3dafd50fbcc63f5",
            "e59d4929a9bb43b483dec9f112b0aa00",
            "0e5085b738254907819dcebcd98806d4",
            "fa667e99f27c4744af0cda5f3e374bc7",
            "9536d3f685b94aa7990e46ce50363766",
            "921eadf0491143cfb7775d9cd364e230",
            "c66b9c595af848dbafac4325ced8bb81"
          ]
        },
        "id": "M-9guOl4ySpg",
        "outputId": "3f9c19f9-71e9-4c71-ef24-bf565b30f582"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Device = cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Ï†ÑÏ≤¥ ÌîåÎû´Ìèº ÏßÑÌñâ (MLP):   0%|          | 0/3 [00:00<?, ?platform/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f9170286d444b7bacfc5d72ea3ccdd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "‚ñ∂ Platform: Coursera\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Optuna Tuning [Coursera]:   0%|          | 0/50 [00:00<?, ?trial/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1306666d4204a808d2bac14ca6c3c4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Best Params: {'hidden_dim': 256, 'depth': 3, 'dropout': 0.4100844240198673, 'lr': 0.0009760516391587593, 'weight_decay': 3.726080253609299e-07, 'batch_size': 256}\n",
            "=== Test Metrics === {'Accuracy': 0.9475656973391547, 'PR_AUC': 0.40514052732582, 'ROC_AUC': 0.8901870524983374, 'F1_score': 0.4299149126735334, 'Best_Threshold': 0.20182031393051147}\n",
            "üìÅ Saved in /content/drive/MyDrive/1014/result_mlp5/Coursera\n",
            "\n",
            "============================================================\n",
            "‚ñ∂ Platform: Audible\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Optuna Tuning [Audible]:   0%|          | 0/50 [00:00<?, ?trial/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16bebce0cfce4f19b5c4c72335141502"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Best Params: {'hidden_dim': 256, 'depth': 4, 'dropout': 0.19522221566144854, 'lr': 0.0017252356782725706, 'weight_decay': 3.636241307304121e-05, 'batch_size': 512}\n",
            "=== Test Metrics === {'Accuracy': 0.9045058608452522, 'PR_AUC': 0.3079070677108698, 'ROC_AUC': 0.7797977166445202, 'F1_score': 0.3616103522645579, 'Best_Threshold': 0.192869633436203}\n",
            "üìÅ Saved in /content/drive/MyDrive/1014/result_mlp5/Audible\n",
            "\n",
            "============================================================\n",
            "‚ñ∂ Platform: Hotel\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Optuna Tuning [Hotel]:   0%|          | 0/50 [00:00<?, ?trial/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ef73c8a6a20480ba59fa0ebb8c98937"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Best Params: {'hidden_dim': 128, 'depth': 3, 'dropout': 0.31045426457642256, 'lr': 0.0014417821799230432, 'weight_decay': 5.660609028019945e-05, 'batch_size': 512}\n",
            "=== Test Metrics === {'Accuracy': 0.8290598290598291, 'PR_AUC': 0.2168572500779447, 'ROC_AUC': 0.701874458962752, 'F1_score': 0.2717753450737744, 'Best_Threshold': 0.16104565560817719}\n",
            "üìÅ Saved in /content/drive/MyDrive/1014/result_mlp5/Hotel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-GIDMqjyWiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
